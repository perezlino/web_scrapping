{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SQLite3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el código que utilizaremos en el archivo **`pipelines.py`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class SQLitePipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # create database file\n",
    "        self.connection = sqlite3.connect('transcripts.db')\n",
    "        # we need a cursor object to execute SQL queries\n",
    "        self.c = self.connection.cursor()\n",
    "        #  try/except will help when running this for the +2nd time (we can't create the same table twice)\n",
    "        try:\n",
    "            # query: create table with columns\n",
    "            self.c.execute('''\n",
    "                CREATE TABLE transcripts(\n",
    "                    title TEXT,\n",
    "                    plot TEXT,\n",
    "                    transcript TEXT,\n",
    "                    url TEXT\n",
    "                )\n",
    "            ''')\n",
    "            # save changes\n",
    "            self.connection.commit()\n",
    "        except sqlite3.OperationalError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.connection.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # query: insert data into table\n",
    "        self.c.execute('''\n",
    "            INSERT INTO transcripts (title,plot,transcript,url) VALUES(?,?,?,?)\n",
    "        ''', (\n",
    "            item.get('title'),\n",
    "            item.get('plot'),\n",
    "            item.get('transcript'),\n",
    "            item.get('url'),\n",
    "        ))\n",
    "        # save changes\n",
    "        self.connection.commit()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editaremos el archivo **`settings.py`**:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/3wrQ2tk5/ws-185.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizará el mismo código que realizamos en la clase pasada en el archivo **`transcripts.py`** (con algunas modificaciones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "\n",
    "class TranscriptsSpider(CrawlSpider):\n",
    "    name = 'transcripts'\n",
    "    allowed_domains = ['subslikescript.com']\n",
    "    # start_urls = ['https://subslikescript.com/movies_letter-X']\n",
    "\n",
    "    # Establecer una variable user agent\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'\n",
    "\n",
    "    # Editar el user agent en la request enviada\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url='https://subslikescript.com/movies_letter-X', headers={\n",
    "            'user-agent':self.user_agent\n",
    "        })\n",
    "\n",
    "    # Establecer reglas para el crawler \n",
    "    rules = (\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"//ul[@class='scripts-list']/a\")), callback='parse_item', follow=True, process_request='set_user_agent'),\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"(//a[@rel='next'])[1]\")), process_request='set_user_agent'),\n",
    "    )\n",
    "\n",
    "    # Configuración del user agent\n",
    "    def set_user_agent(self, request, spider):\n",
    "        request.headers['User-Agent'] = self.user_agent\n",
    "        return request\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        # Obtener la caja del article que contiene los datos que queremos (title, plot, etc)\n",
    "        article = response.xpath(\"//article[@class='main-article']\")\n",
    "        # .getall() devolverá una lista, usa .join() para convertir la lista en un string (ESTO SE AGREGÓ !!)\n",
    "        transcript_list = article.xpath(\"./div[@class='full-script']/text()\").getall()\n",
    "        transcript_string = ' '.join(transcript_list)\n",
    "\n",
    "        # Extraer los datos que queremos y luego devolverlos\n",
    "        yield {\n",
    "            'title':article.xpath(\"./h1/text()\").get(),\n",
    "            'plot':article.xpath(\"./p/text()\").get(),\n",
    "            'transcript':transcript_string,\n",
    "            'url':response.url,\n",
    "            # 'user-agent':response.request.headers['User-Agent'],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos desde el terminal:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/wj2jKyJz/ws-186.png\"></center>\n",
    "\n",
    "Se creará el archivo **`transcripts.db`**:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pddXPWYF/ws-187.png\"></center>\n",
    "\n",
    "El cual podemos analizarlo en el sitioweb:\n",
    "\n",
    "https://inloop.github.io/sqlite-viewer/\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZqDYRGVD/ws-188.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
