{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cambio de User-Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el código que utilizamos para configurar el **`User Agent`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "\n",
    "class TranscriptsSpider(CrawlSpider):\n",
    "    name = 'transcripts'\n",
    "    allowed_domains = ['subslikescript.com']\n",
    "    # start_urls = ['https://subslikescript.com/movies_letter-X']\n",
    "\n",
    "    # Establecer una variable user agent\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "    # Editar el user agent en la request enviada\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url='https://subslikescript.com/movies_letter-X', headers={\n",
    "            'user-agent':self.user_agent\n",
    "        })\n",
    "\n",
    "    # Establecer reglas para el crawler \n",
    "    rules = (\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"//ul[@class='scripts-list']/a\")), callback='parse_item', follow=True, process_request='set_user_agent'),\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"(//a[@rel='next'])[1]\")), process_request='set_user_agent'),\n",
    "    )\n",
    "\n",
    "    # Configuración del user agent\n",
    "    def set_user_agent(self, request, spider):\n",
    "        request.headers['User-Agent'] = self.user_agent\n",
    "        return request\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        # Obtener la caja del article que contiene los datos que queremos (title, plot, etc)\n",
    "        article = response.xpath(\"//article[@class='main-article']\")\n",
    "\n",
    "        # Extraer los datos que queremos y luego devolverlos\n",
    "        yield {\n",
    "            'title': article.xpath(\"./h1/text()\").get(),\n",
    "            'plot': article.xpath(\"./p/text()\").get(),\n",
    "            #'transcript': article.xpath(\"./div[@class='full-script']/text()\").getall(),\n",
    "            'url': response.url,\n",
    "            'user-agent': response.request.headers['User-Agent'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, si queremos extraerlo en un archivo **`CSV`**:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Mp9qT0n3/ws-158.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/tg7yV1Pq/ws-159.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
