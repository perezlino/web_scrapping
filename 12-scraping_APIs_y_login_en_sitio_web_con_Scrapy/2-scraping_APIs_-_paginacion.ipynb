{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scraping APIs - Paginación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si inspeccionamos la página vemos todo tal cual la clase anterior:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/hjhnZZQ7/ws-199.png\"></center>\n",
    "\n",
    "Sin embargo, si scroleamos hacia abajo en el sitio nos aparece una segunda página:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/sX8rkP5H/ws-200.png\"></center>\n",
    "\n",
    "Una tercera y asi sucesivamente:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/XqCb4Z8j/ws-201.png\"></center>\n",
    "\n",
    "Pero vemos que solo tiene 10 páginas:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/K4hyzZqn/ws-202.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código que utilizaremos en el archivo **`quotes.py`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['https://quotes.toscrape.com/api/quotes?page=1']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Almacenar la respuesta en json y obtener quotes\n",
    "        json_response = json.loads(response.body)\n",
    "        quotes = json_response.get('quotes')\n",
    "\n",
    "        # Recorrer los elementos de quotes\n",
    "        for quote in quotes:\n",
    "            # Devuelve los datos extraídos\n",
    "            yield {\n",
    "                'author': quote.get('author').get('name'),\n",
    "                'tags': quote.get('tags'),\n",
    "                'quotes': quote.get('text'),\n",
    "            }\n",
    "\n",
    "        # Escoger el elemento \"has_next\"\n",
    "        has_next = json_response.get('has_next')\n",
    "\n",
    "        # Si has_next==True (hay página siguiente), ejecuta el siguiente código\n",
    "        if has_next:\n",
    "            next_page_number = json_response.get('page')+1\n",
    "            yield scrapy.Request(\n",
    "                url=f'https://quotes.toscrape.com/api/quotes?page={next_page_number}',\n",
    "                callback=self.parse\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos desde la consola y extraemos la data a un archivo **`JSON`**:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pTtVGMMc/ws-203.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/Ss7SH1yS/ws-204.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
