{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scraping del sitio web con scrolling infinito**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo visto Twitter sólo carga los tweets de la página en la que estamos en ese momento. Así que esto significa que sólo obtendremos los tweets de la página en la que estemos en el momento en que hagamos el scraping. Es decir, tendremos que scrapear cada página a medida que scrolleamos hacia abajo y no esperar hasta el final para scrapearlo todo. Desafortunadamente, una de las desventajas de este enfoque es que podríamos obtener tweets duplicados porque Selenium podría capturar elementos que ya ha localizado de las páginas anteriores. Vamos a manejar estos tweets duplicados con sets (conjuntos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, si buscamos un usuario en la pagina del tweet que estamos trabajando, al principio si lo encuentra. Pero, a medida que scrolleamos hacia abajo (y se va cargando la página) ya no encuentra a ese usuario.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/3JxbdYsg/ws-33.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/ncSSmNxN/ws-34.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diseño de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "website = 'https://twitter.com/Support/status/1600561455488970752'\n",
    "driver = webdriver.Chrome(executable_path = r'C:\\Users\\Alfonso\\Downloads\\Programas\\chromedriver\\chromedriver.exe')\n",
    "driver.get(website)\n",
    "driver.maximize_window()\n",
    "\n",
    "def get_tweet(element):\n",
    "    \"\"\"Esta función scrapea datos de tweets. Devuelve una lista con 2 elementos; nombre de usuario y texto\"\"\"\n",
    "    try:\n",
    "        user = element.find_element_by_xpath(\".//span[contains(text(), '@')]\").text  # Obtener el nombre de usuario\n",
    "        text = element.find_element_by_xpath(\".//div[@lang]\").text  # Obtener el texto de la tarjeta\n",
    "        tweets_data = [user, text]\n",
    "    except:\n",
    "        tweets_data = ['user', 'text']\n",
    "    return tweets_data\n",
    "\n",
    "\n",
    "user_data = []\n",
    "text_data = []\n",
    "tweet_ids = set()\n",
    "scrolling = True\n",
    "while scrolling:\n",
    "    tweets = WebDriverWait(driver, 5).until(EC.presence_of_all_elements_located((By.XPATH, \"//article[@role='article']\")))\n",
    "    print(len(tweets))\n",
    "    for tweet in tweets[-15:]:  # puedes cambiar este número con el número de tweets en un sitio web || NOTA: SÓLO SE CONSIDERARÁN \n",
    "                                # AQUELLOS CARGADOS EN LA ÚLTIMA PÁGINA mientras que se olvidarán los de la página anterior (ejemplo: \n",
    "                                # desplácese hasta abajo y luego intente encontrar un @nombredeusuario que esté arriba --> no lo encontrará)\n",
    "        tweet_list = get_tweet(tweet)\n",
    "        tweet_id = ''.join(tweet_list)\n",
    "        if tweet_id not in tweet_ids:\n",
    "            tweet_ids.add(tweet_id)\n",
    "            user_data.append(tweet_list[0])\n",
    "            text_data.append(\" \".join(tweet_list[1].split()))\n",
    "\n",
    "    # Obtener la altura inicial del scroll\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:  # Siempre se ejecutará este loop hasta al momento que sea \"False\" o se ejecute el \"break\"\n",
    "        # Scroll hacia abajo hasta la parte inferior\n",
    "        # Este argumento \"document.body.scrollHeight\" nos indica la tamaño(size)/altura(height) de la página. Si escribimos este comando\n",
    "        # nos servirá para cualquier pantalla (1080, etc.)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Esperar para cargar la página\n",
    "        time.sleep(5)\n",
    "        # Calcular la nueva altura del scroll y compararla con la última altura del scroll\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:  # si la altura nueva y la última son iguales, significa que no hay ninguna página nueva que cargar, así que dejamos de hacer scroll\n",
    "            scrolling = False\n",
    "            break\n",
    "        # condition 2\n",
    "        # if len(data) > 60:\n",
    "        #     scrolling = False\n",
    "        #     break\n",
    "        else:\n",
    "            last_height = new_height\n",
    "            break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df_tweets = pd.DataFrame({'user': user_data, 'text': text_data})\n",
    "df_tweets.to_csv('tweets_pagination.csv', index=False)\n",
    "print(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "abecedario = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "for letra in abecedario[-5:]:\n",
    "    print(letra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, esto es lo que hace el código:\n",
    "\n",
    "1. Ingresa al sitio web especifico de un tweet\n",
    "2. Va a tomar los últimos 15 comentarios (podemos modificar este número a gusto)\n",
    "3. Luego para cada comentario se crea un ID, uniendo todo el usuario y texto\n",
    "4. Luego, para evitar la duplicacion de un comentario, se utiliza un IF para ir comparando los ID\n",
    "5. Si no, existe se agrega a un conjunto (no permite duplicion de un elemento)\n",
    "6. Luego, se agrega la data en user_data y text_data\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/6Q3BDvj2/ws-35.png\"></center>\n",
    "\n",
    "7. Y luego scrolea hacia al final de la página y se cargan nuevos comentarios\n",
    "8. Si mantiene el mismo tamaño/altura la página se rompen el primer y segundo while y genera el dataframe y el archivo CSV\n",
    "9. Si no mantiene el mismo tamaño/altura la página (se cargan nuevos comentarios) nos saca del segundo while \n",
    "10. Se repiten los pasos del 2 al 6\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/HnGHcPpX/ws-36.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
