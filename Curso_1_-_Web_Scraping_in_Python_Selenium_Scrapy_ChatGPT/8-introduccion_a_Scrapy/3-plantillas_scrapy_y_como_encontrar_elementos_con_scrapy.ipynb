{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plantillas Scrapy y cómo encontrar elementos con Scrapy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Te explicaré algunas cosas que necesitas saber sobre las plantillas que usamos para scrapear sitios web con Scrapy, y también te mostraré cómo encontrar elementos con Scrapy. En Scrapy hay dos plantillas populares: \n",
    "\n",
    "- \"**`scrapy.Spider`**\" y \n",
    "- \"**`CrawlSpider`**\" \n",
    "\n",
    "La \"**`scrapy.Spider`**\" es la spider más simple y la que usaremos más a menudo en este curso. No proporciona ninguna funcionalidad especial, pero podemos personalizar esta plantilla para hacer scraping de la forma que queramos. Por otro lado, la \"**`CrawlSpider`**\" es la spider más utilizada para hacer crawling de sitios web normales. Proporciona algunos mecanismos para seguir enlaces definiendo un conjunto de reglas. Tenga en cuenta que \"**`crawling`**\" no es lo mismo que \"**`scraping`**\" en un sitio web. Un \"crawler\" normalmente navega por la \"World Wide Web (www)\" con el propósito de indexar la web. Pero el \"web scraping\" consiste más bien en extraer información de sitios web. Así que un \"CrawlSpider\" puede no ser el más adecuado para su proyecto de web scraping.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/c4JwtRgF/ws-46.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora la plantilla \"**`scrapy.Spider`**\":\n",
    "\n",
    "Así que esta plantilla tiene por defecto una clase con el nombre de nuestra spider. Aquí enviaremos requests desde el atributo spider \"**`start_urls`**\" y llamaremos a este método spider \"**`parse`**\" para cada una de las responses resultantes. Así que aquí vamos a hacer todos los datos estructurados. Se verá esta plantilla en acción en los siguientes videos. Pero ahora es el momento de ver cómo encontrar elementos con Scrapy.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/qMZbNMRC/ws-47.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que en un Scrapy, usamos \"**`response`**\" para encontrar elementos. **`Esto es igual que el \"driver\" en Selenium o el \"soup\" en BeautifulSoup`**. Esta \"response\" representa la respuesta que obtenemos después de enviar requests a un sitio web. Ahora, **`a diferencia de Selenium, sólo podemos encontrar elementos con XPath en Scrapy`**. Scrapy no tiene funciones como \"find_element_by_id\" o \"class names\" o \"tag names\". Pero aún podemos encontrar estos elementos escribiendo un XPath equivalente. Así que para encontrar elementos con XPath en Scrapy, tenemos que escribir: \n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/DyPSy8w7/ws-48.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora al igual que en Selenium o en BeautifulSoup, podemos encontrar múltiples elementos con Scrapy. En este caso tenemos que usar \"**`get_all()`**\" para obtener todos los elementos con el mismo XPath. Y aquí también, si usas \"**`get_all()`**\" obtendrás una lista con todos los elementos que coincidan con el XPath dentro de la lista. Pero si utilizas \"**`get()`**\", sólo obtendrás un único elemento.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/V69FfzSp/ws-49.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, verás más adelante que utilizaremos mucho la palabra clave \"**`yield`**\". No te asustes si nunca has trabajado con \"**`yield`**\" antes, porque funciona de la misma manera que la palabra clave \"**`return`**\" cuando defines una función. Sin embargo, con la palabra clave \"**`yield`**\", podemos devolver valores de una función sin destruir el estado de su variable local.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/gkyKFVXs/ws-50.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
